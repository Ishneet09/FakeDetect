{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Major Project final .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ededf574ae64d03a344edd3e2f11fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da62e505be7c435e9af2bea599992ac2",
              "IPY_MODEL_4d07a76269e94fef9a01f058fd0aa4a2"
            ],
            "layout": "IPY_MODEL_7f344d3051dd466ea0cce1b22e888499"
          }
        },
        "da62e505be7c435e9af2bea599992ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f7c51eb31b94d24b51aba18343a0585",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f02f6072955441c9b067cd11fad7c76",
            "value": 0
          }
        },
        "4d07a76269e94fef9a01f058fd0aa4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f41bcae14bca49999a4835ee86116e85",
            "placeholder": "​",
            "style": "IPY_MODEL_7695b1b118cf4f81bf130abbc32b9f25",
            "value": " 0/? [00:00&lt;?, ?it/s]"
          }
        },
        "7f344d3051dd466ea0cce1b22e888499": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f7c51eb31b94d24b51aba18343a0585": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f02f6072955441c9b067cd11fad7c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "f41bcae14bca49999a4835ee86116e85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7695b1b118cf4f81bf130abbc32b9f25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh5IwpnLSPPG"
      },
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "from tensorflow.keras.applications import ResNet50\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLgBgrRbUaBb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txEOqFumTa__"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AztxzqKISPPb"
      },
      "source": [
        "import tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGFttpT0TCRB",
        "outputId": "9cc3985f-67ff-491f-9145-d4ca2e06fb9c"
      },
      "source": [
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W8N1ckASPPe",
        "outputId": "cb16f520-3633-4a0f-950c-a61c0adc9a79"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0ededf574ae64d03a344edd3e2f11fe5",
            "da62e505be7c435e9af2bea599992ac2",
            "4d07a76269e94fef9a01f058fd0aa4a2",
            "7f344d3051dd466ea0cce1b22e888499",
            "0f7c51eb31b94d24b51aba18343a0585",
            "7f02f6072955441c9b067cd11fad7c76",
            "f41bcae14bca49999a4835ee86116e85",
            "7695b1b118cf4f81bf130abbc32b9f25"
          ]
        },
        "id": "TvVDuztGSPPg",
        "outputId": "46d1111b-5f2c-4fd5-a0ca-8ab0f347364a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "tqdm().pandas()\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import cv2\n",
        "from os import listdir\n",
        "from sklearn.metrics import accuracy_score\n",
        "!pip install tensorflow_hub\n",
        "!pip install bert-tensorflow==1.0.1\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "import tensorflow_hub as hub  #pip install tensorflow_hub\n",
        "import os\n",
        "from bert import tokenization\n",
        "from bert.tokenization import FullTokenizer\n",
        "from tqdm import tqdm_notebook\n",
        "from tensorflow.compat.v1.keras import backend as K\n",
        "# from keras import backend as K\n",
        "# Initialize session\n",
        "# sess = tf.Session()\n",
        "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) #command to run codeon multiple gpu\n",
        "\n",
        "\n",
        "\n",
        "# Params for bert model and tokenization\n",
        "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "!pip install keras-lr-finder\n",
        "from keras_lr_finder import LRFinder\n",
        "!pip install talos as ta\n",
        "import talos as ta\n",
        "from pprint import pprint\n",
        "from talos.utils import live\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "from random import choice\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ededf574ae64d03a344edd3e2f11fe5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (56.1.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n",
            "Collecting bert-tensorflow==1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow==1.0.1) (1.15.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n",
            "Device mapping: no known devices.\n",
            "Collecting keras-lr-finder\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/f8/6668401e734e90eb4e63c41d30e9ae895355e1e078370be32355253b03a0/keras_lr_finder-0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from keras-lr-finder) (2.4.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from keras-lr-finder) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->keras-lr-finder) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->keras-lr-finder) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->keras-lr-finder) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->keras-lr-finder) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-lr-finder) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-lr-finder) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-lr-finder) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-lr-finder) (0.10.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.0->keras-lr-finder) (1.5.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->keras-lr-finder) (1.15.0)\n",
            "Installing collected packages: keras-lr-finder\n",
            "Successfully installed keras-lr-finder-0.1\n",
            "Collecting talos\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/90/2455415b2a2170ad649b66d79ea74ff1af546c012836a2b621323a5fabfd/talos-1.0-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.4MB/s \n",
            "\u001b[?25hCollecting as\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/08/226c133ec497d25a63edb38527c02db093c7d89e6d4cdc91078834486a5d/as-0.1-py3-none-any.whl\n",
            "Collecting ta\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/22/a355ecf2d67da8150332d22ef65c3a1f79109528279bf5d40735b6f2bd72/ta-0.7.0.tar.gz\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from talos) (1.1.5)\n",
            "Collecting statsmodels>=0.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/69/8eef30a6237c54f3c0b524140e2975f4b1eea3489b45eb3339574fc8acee/statsmodels-0.12.2-cp37-cp37m-manylinux1_x86_64.whl (9.5MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5MB 19.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from talos) (2.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from talos) (2.23.0)\n",
            "Collecting kerasplotlib\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/b7/31663d3b5ea9afd8c2c6ffa06d3c4e118ef363e12dc75b7c49fb6a2d22aa/kerasplotlib-0.1.6.tar.gz\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from talos) (4.41.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from talos) (0.0)\n",
            "Collecting chances\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/d8/d61112d7476dc3074b855f1edd8556cde9b49b7106853f0b060109dd4c82/chances-0.1.9.tar.gz\n",
            "Collecting wrangle\n",
            "  Downloading https://files.pythonhosted.org/packages/85/35/bc729e377417613f2d062a890faea5d649ef1a554df21499e9c3a4a5501a/wrangle-0.6.7.tar.gz\n",
            "Collecting astetik\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/c1/b40ed42915c66d54728a6059de0d35088b329a677f01e9c6f50a71b5b361/astetik-1.11.1-py3-none-any.whl (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 23.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from talos) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->talos) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->talos) (2.8.1)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11.0->talos) (0.5.1)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11.0->talos) (1.4.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (3.12.4)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (0.4.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (2.5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (3.3.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.34.1)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (0.2.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.12)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (0.12.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.12.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->talos) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->talos) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->talos) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->talos) (2.10)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from kerasplotlib->talos) (5.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->talos) (0.22.2.post1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from wrangle->talos) (2.4.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from astetik->talos) (0.11.1)\n",
            "Collecting geonamescache\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/ba/b7939087621bfeb24c0f52c4b879865a9f902cda72efd119f4275400e692/geonamescache-1.2.0-py3-none-any.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 27.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=2.0.0->talos) (56.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (1.30.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (0.6.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.0.0->talos) (1.5.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->kerasplotlib->talos) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->kerasplotlib->talos) (5.0.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->kerasplotlib->talos) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->kerasplotlib->talos) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->kerasplotlib->talos) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->kerasplotlib->talos) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->kerasplotlib->talos) (0.7.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->talos) (1.0.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->wrangle->talos) (3.13)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn->astetik->talos) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.0.0->talos) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0.0->talos) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0.0->talos) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0.0->talos) (4.7.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.0.0->talos) (4.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kerasplotlib->talos) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->kerasplotlib->talos) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->kerasplotlib->talos) (0.7.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->astetik->talos) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->astetik->talos) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->astetik->talos) (2.4.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.0.0->talos) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0.0->talos) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.0.0->talos) (3.4.1)\n",
            "Building wheels for collected packages: ta, kerasplotlib, chances, wrangle\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.7.0-cp37-none-any.whl size=28716 sha256=067aa7571ebe306229d5fa54f1107f8fe3a6dbaf7bb5679c2bc289f83fc1b457\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/88/30/de9553fb54a474eb7480b937cdbb140bdda613d29cf4da7994\n",
            "  Building wheel for kerasplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kerasplotlib: filename=kerasplotlib-0.1.6-cp37-none-any.whl size=3603 sha256=517e677adaca37617fa43e07f503813e3c5b65873905103c8a8fc79a0d73e51d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/d3/8c/9503a22b0a38e8b21c70ad834e4606d209193443e5c709305d\n",
            "  Building wheel for chances (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chances: filename=chances-0.1.9-cp37-none-any.whl size=41610 sha256=9a3698e214444ed14fb3b7c409f43a05185409b0cc18bbb4ece78d37a12dcae3\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/33/46/c871b94249bd57d17797d049b3dff8e3a09c315afb67eb14c6\n",
            "  Building wheel for wrangle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrangle: filename=wrangle-0.6.7-cp37-none-any.whl size=49894 sha256=3aeae7bc529fbfb4ab77b760b2562a0b91e71726e631615bbd1c2ad6344cb0c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/1b/50/d0403ce6ef269e364894da7b50db68db14c4ac62c577561e2d\n",
            "Successfully built ta kerasplotlib chances wrangle\n",
            "\u001b[31mERROR: wrangle 0.6.7 has requirement scipy==1.2, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: statsmodels, kerasplotlib, chances, wrangle, geonamescache, astetik, talos, as, ta\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "Successfully installed as-0.1 astetik-1.11.1 chances-0.1.9 geonamescache-1.2.0 kerasplotlib-0.1.6 statsmodels-0.12.2 ta-0.7.0 talos-1.0 wrangle-0.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM3FuTyMSPPi",
        "outputId": "ef466b68-cee3-4dd7-9017-a2f4a96e1488"
      },
      "source": [
        "!pip install tensorflow_hub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (56.1.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzmd352lSPPj"
      },
      "source": [
        "from talos.model.normalizers import lr_normalizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCYH1V6XSPPk"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSMKuxmBSPPl"
      },
      "source": [
        "def get_df(file):\n",
        "    return pd.read_csv(file,sep = '\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_diPsqj6SPPm"
      },
      "source": [
        "train_df = get_df('/content/drive/MyDrive/mediaeval2015/train_posts.txt')\n",
        "test_df = get_df('/content/drive/MyDrive/mediaeval2015/test_posts.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LQMTav_UJpI"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeSMBEqESPPn"
      },
      "source": [
        "def return_first_image(row):\n",
        "    return row['imageId(s)'].split(',')[0].strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esGTit2DSPPo"
      },
      "source": [
        "train_df['first_image_id'] = train_df.progress_apply (lambda row: return_first_image(row),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeDQ-lxOSPPp"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJhkiDTySPPq"
      },
      "source": [
        "test_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAkob1n3SPPq"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fdFlPsNSPPr"
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He0NkrbQSPPr"
      },
      "source": [
        "train_df['first_image_id'] = train_df.progress_apply (lambda row: return_first_image(row),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60C8ec3oSPPs"
      },
      "source": [
        "images_train_dataset = [i for i in train_df['first_image_id'].tolist()]\n",
        "images_train_folder = [i.split('.')[0].strip() for i in listdir('/content/drive/MyDrive/mediaeval2015/images_train')]\n",
        "images_train_not_available = set(images_train_dataset)-set(images_train_folder)\n",
        "images_train_not_available"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCo0O8fZSPPs"
      },
      "source": [
        "\n",
        "images_test_dataset = [i.split(',')[0].strip() for i in test_df['imageId(s)'].tolist()]\n",
        "images_test_folder = [i.split('.')[0].strip() for i in listdir('/content/drive/MyDrive/mediaeval2015/images_test')]\n",
        "images_test_not_available = set(images_test_dataset)-set(images_test_folder)\n",
        "images_test_not_available"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dEf48cKISPPt"
      },
      "source": [
        "train_df = train_df[~train_df['first_image_id'].isin(images_train_not_available)]\n",
        "test_df = test_df[~test_df['imageId(s)'].isin(images_test_not_available)]\n",
        "\n",
        "train_text = train_df['tweetText'].tolist()\n",
        "test_text = test_df['tweetText'].tolist()\n",
        "\n",
        "train_images = [i for i in train_df['first_image_id'].tolist()]\n",
        "test_images = [i.split(',')[0].strip() for i in test_df['imageId(s)'].tolist()]\n",
        "\n",
        "trainY = train_df['label'].tolist()\n",
        "trainY = [1 if i=='real' else 0 for i in trainY]\n",
        "\n",
        "testY = test_df['label'].tolist()\n",
        "testY = [1 if i=='real' else 0 for i in testY]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Evnq9f_TYwCj"
      },
      "source": [
        "print(trainY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6IoOoHOZc69x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "udufrYOYaqn0"
      },
      "source": [
        "print(trainY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JDi0cL1ReTtY"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cCiQMhoCSPPt"
      },
      "source": [
        "len(train_text),len(train_images),len(trainY),len(test_text),len(test_images),len(testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nkU6CZN-SPPu"
      },
      "source": [
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "    return max([len(s.split()) for s in lines])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0U86IKJUe4_9"
      },
      "source": [
        "print(trainY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sU_3p88cbQ2l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5u0jyGC9SPPv"
      },
      "source": [
        "print(\"maximum length:\", max_length(train_text))\n",
        "plt.hist([len(s.split()) for s in train_text],bins=[0,1,2,3,4,5,6,7,8,9,12,15,16,18,20,25,30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jlMryUpCe9tp"
      },
      "source": [
        "print(trainY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LHepzXMBSPPw"
      },
      "source": [
        "l=[len(s.split()) for s in train_text]\n",
        "count=0.0\n",
        "for i in l:\n",
        "    if i>22:\n",
        "        count+=1\n",
        "print(count,len(l))\n",
        "print(count/len(l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zpEquwL4SPPx"
      },
      "source": [
        "max_seq_length=23"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ln8vBzgEbIcE"
      },
      "source": [
        "print(trainY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iCDnCJomSPPy"
      },
      "source": [
        "class PaddingInputExample(object):\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_module =  hub.Module(bert_path)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    vocab_file, do_lower_case = sess.run(\n",
        "        [\n",
        "            tokenization_info[\"vocab_file\"],\n",
        "            tokenization_info[\"do_lower_case\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        input_ids = [0] * max_seq_length\n",
        "        input_mask = [0] * max_seq_length\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        label = 0\n",
        "        return input_ids, input_mask, segment_ids, label\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    return input_ids, input_mask, segment_ids, example.label\n",
        "\n",
        "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
        "        input_id, input_mask, segment_id, label = convert_single_example(\n",
        "            tokenizer, example, max_seq_length\n",
        "        )\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        labels.append(label)\n",
        "    return (\n",
        "        np.array(input_ids),\n",
        "        np.array(input_masks),\n",
        "        np.array(segment_ids),\n",
        "        np.array(labels).reshape(-1, 1),\n",
        "    )\n",
        "\n",
        "def convert_text_to_examples(texts, labels):\n",
        "    \"\"\"Create InputExamples\"\"\"\n",
        "    InputExamples = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        InputExamples.append(\n",
        "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
        "        )\n",
        "    return InputExamples\n",
        "\n",
        "# Instantiate tokenizer\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ouS803YUbBuP"
      },
      "source": [
        "print(trainY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-hOMmzshOuc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "guYfjeX_SPP2"
      },
      "source": [
        "# Convert data to InputExample format\n",
        "train_examples = convert_text_to_examples(train_text, trainY)\n",
        "test_examples = convert_text_to_examples(test_text, testY)\n",
        "\n",
        "# Convert to features\n",
        "(train_input_ids, train_input_masks, train_segment_ids, trainY \n",
        ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
        "(test_input_ids, test_input_masks, test_segment_ids, testY\n",
        ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9rjeLopFfNKf"
      },
      "source": [
        "print(trainY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_XT9Q4XxSPP5"
      },
      "source": [
        "length = 224\n",
        "width = 224\n",
        "channels = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "432UcP9HSPP5"
      },
      "source": [
        "def read_and_process_image(list_of_images):\n",
        "    X = [] \n",
        "    for image in tqdm(list_of_images):\n",
        "        try:\n",
        "           X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (length,width), interpolation=cv2.INTER_CUBIC))  \n",
        "        except:\n",
        "          print(image)\n",
        "            \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cxNBtoJLSPP6"
      },
      "source": [
        "images = listdir('/content/drive/MyDrive/mediaeval2015/images_train')\n",
        "images.extend(listdir('/content/drive/MyDrive/mediaeval2015/images_test'))\n",
        "jpg = []\n",
        "png=[]\n",
        "jpeg=[]\n",
        "gif = []\n",
        "\n",
        "for i in images:\n",
        "    name,ext = i.split('.')[0],i.split('.')[-1]\n",
        "    #print(name)\n",
        "    #print(ext)\n",
        "    #print(eval(ext))\n",
        "    #eval(ext).append(name)\n",
        "    if(ext == \"jpg\"):\n",
        "      jpg.append(name)\n",
        "    if(ext == \"jpeg\"):\n",
        "      jpeg.append(name)\n",
        "    if(ext == \"gif\"):\n",
        "      gif.append(name)\n",
        "    if(ext == \"png\"):\n",
        "      png.append(name)\n",
        "#print(jpg)\n",
        "#print(png)\n",
        "#print(gif)\n",
        "#print(jpeg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_gghLDmfSPP8"
      },
      "source": [
        "def get_extension_of_file(file_name):\n",
        "    if file_name in jpg:\n",
        "        return '.jpg'\n",
        "    elif file_name in png:\n",
        "        return '.png'\n",
        "    elif file_name in jpeg:\n",
        "        return '.jpeg'\n",
        "    else:\n",
        "        return '.gif'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0WZ3FcKzSPP9"
      },
      "source": [
        "train_images = ['/content/drive/MyDrive/mediaeval2015/images_train/'+i+get_extension_of_file(i) for i in train_images]\n",
        "test_images = ['/content/drive/MyDrive/mediaeval2015/images_test/'+i+get_extension_of_file(i) for i in test_images]\n",
        "\n",
        "print(train_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yf3tTGVLu6-P"
      },
      "source": [
        "print(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xi0gws3h2HQl"
      },
      "source": [
        "train_imagesX = read_and_process_image(train_images)\n",
        "test_imagesX = read_and_process_image(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E2TJ-QiKSPP-"
      },
      "source": [
        "np.save('train_imagesX', train_images)\n",
        "np.save('test_imagesX', test_images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x7Pi4Fe-OcNk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LggX9_W9SPP-"
      },
      "source": [
        "train_imagesX = np.load('train_imagesX.npy')\n",
        "test_imagesX = np.load('test_imagesX.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cYgzNkycyYuS"
      },
      "source": [
        "print(train_imagesX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xvno9x2WrVCL"
      },
      "source": [
        "print(train_imagesX.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8bxa6TNWxsoH"
      },
      "source": [
        "#train_imagesX = np.rollaxis(train_imagesX, 3, 1)\n",
        "#test_imagesX = np.rollaxis(test_imagesX,3,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t3jBYd5pSPP_"
      },
      "source": [
        "class BertLayer(tf.compat.v1.keras.layers.Layer):\n",
        "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            bert_path,\n",
        "            trainable=self.trainable,\n",
        "            name=\"{}_module\".format(self.name)\n",
        "        )\n",
        "\n",
        "        trainable_vars = self.bert.variables\n",
        "\n",
        "        # Remove unused layers\n",
        "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "\n",
        "        # Select how many layers to fine tune\n",
        "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
        "\n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "            \n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "            \"pooled_output\"\n",
        "        ]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "coApFhk-SPP_"
      },
      "source": [
        "def initialize_vars(sess):\n",
        "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) #command to run codeon multiple gpu\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uv2fXpxnSPP_"
      },
      "source": [
        "# base.trainable=False\n",
        "first = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ajPR0dAnODNw"
      },
      "source": [
        "param1 = [train_input_ids, train_input_masks, train_segment_ids, train_imagesX]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9I5tVsHzOIeG"
      },
      "source": [
        "params={'bert_trainable' :[False,True],\n",
        "    'text_no_hidden_layer':(0,3,3),\n",
        "    'text_hidden_neurons':[768,400,32],\n",
        "    'dropout':(0.3,0.7,4),\n",
        "    'repr_size':[32],\n",
        "    'vis_no_hidden_layer':(0,3,3),\n",
        "    'vis_hidden_neurons':[4096,2742,1388,32],\n",
        "    'final_no_hidden_layer':(0,3,3),\n",
        "    'final_hidden_neurons':[64,35,5],\n",
        "    'optimizer':['adam','rmsprop'],\n",
        "    'batch_size':[512],\n",
        "    'epochs':[10],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XSCu3XrgONA0"
      },
      "source": [
        "#text model\n",
        "def news_model(x_train, y_train, x_val, y_val, params):\n",
        "    \n",
        "#     pprint(params)\n",
        "    try:\n",
        "        del model\n",
        "    except:\n",
        "        pass\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "    \n",
        "    with tf.device('/cpu:0'):\n",
        "        bert_base = BertLayer()\n",
        "        bert_base.trainable= params['bert_trainable']\n",
        "\n",
        "        in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "        in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "        in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "        bert_inputs = [in_id, in_mask, in_segment]\n",
        "        bert_output = bert_base(bert_inputs)\n",
        "\n",
        "        if params['text_no_hidden_layer']>0:\n",
        "            for i in range(params['text_no_hidden_layer']):\n",
        "                bert_output = tf.keras.layers.Dense(params['text_hidden_neurons'], activation='relu')(bert_output)\n",
        "                bert_output = tf.keras.layers.Dropout(params['dropout'])(bert_output)\n",
        "\n",
        "        text_repr = tf.keras.layers.Dense(params['repr_size'], activation='relu')(bert_output)\n",
        "\n",
        "        #image model\n",
        "        conv_base = tf.keras.applications.ResNet50(input_shape=(224,224,3), include_top=False, weights=\"imagenet\")\n",
        "        conv_base.trainable=False\n",
        "#         conv_base = base\n",
        "\n",
        "        input_image = tf.keras.layers.Input(shape=(224,224,3))\n",
        "        base_output = conv_base(input_image)\n",
        "        flat = tf.keras.layers.Flatten()(base_output)\n",
        "\n",
        "        if params['vis_no_hidden_layer']>0:\n",
        "            for i in range(params['vis_no_hidden_layer']):\n",
        "                flat = tf.keras.layers.Dense(params['vis_hidden_neurons'], activation='relu')(flat)\n",
        "                flat = tf.keras.layers.Dropout(params['dropout'])(flat)\n",
        "\n",
        "        visual_repr = tf.keras.layers.Dense(params['repr_size'],activation='relu')(flat)\n",
        "\n",
        "\n",
        "        #classifier\n",
        "        combine_repr = tf.keras.layers.concatenate([text_repr, visual_repr])\n",
        "        com_drop=tf.keras.layers.Dropout(params['dropout'])(combine_repr)\n",
        "\n",
        "        if params['final_no_hidden_layer']>0:\n",
        "            for i in range(params['final_no_hidden_layer']):\n",
        "                com_drop = tf.keras.layers.Dense(params['final_hidden_neurons'], activation='relu')(com_drop)\n",
        "                com_drop=tf.keras.layers.Dropout(params['dropout'])(com_drop)\n",
        "\n",
        "        prediction = tf.keras.layers.Dense(1,activation='sigmoid')(com_drop)\n",
        "\n",
        "        model = tf.keras.models.Model(inputs=[in_id,in_mask,in_segment,input_image], outputs=prediction)\n",
        "\n",
        "   # model = tf.keras.utils.multi_gpu_model(model,gpus=1)\n",
        "    \n",
        "#     if params['optimizer'] == 'adam':\n",
        "#         opt = tf.keras.optimizers.Adam(lr=0.0005)\n",
        "#     else:\n",
        "#         opt = tf.keras.optimizers.RMSprop(lr=0.00005)\n",
        "        \n",
        "#     model.compile(loss='binary_crossentropy', optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])), metrics=['accuracy'])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=params['optimizer'], metrics=['accuracy'])\n",
        "    initialize_vars(sess)\n",
        "    \n",
        "    steps_per_epoch = 1\n",
        "    \n",
        "    out = model.fit(x=param1, y=trainY,\n",
        "                    batch_size=params['batch_size'],\n",
        "                    epochs=params['epochs'],\n",
        "                    verbose=0,\n",
        "                    shuffle=True,\n",
        "                    validation_data=[x_val, y_val],callbacks=[live()])\n",
        "    \n",
        "    return out, model\n",
        "\n",
        "h = ta.Scan(x=param1, y=trainY, params=params,\n",
        "            model=news_model,\n",
        "            experiment_name='twitter_fake_news'\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDf2Gdn8gLUp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ElhwMvHqSPQB"
      },
      "source": [
        "r = ta.Reporting(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YCz3plnzSPQC"
      },
      "source": [
        "r.high('val_acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XlC9FpWnSPQC"
      },
      "source": [
        "r.plot_corr()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TXyiKtjhSPQD"
      },
      "source": [
        "for k in params.keys():\n",
        "    print(k)\n",
        "    r.plot_box(y='val_acc',x=k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s3NNA-F_SPQD"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X = r.data[['bert_trainable','text_no_hidden_layer','text_hidden_neurons','dropout','vis_no_hidden_layer','vis_hidden_neurons','final_no_hidden_layer','final_hidden_neurons','optimizer']]\n",
        "scaler = MinMaxScaler()\n",
        "y = scaler.fit_transform(r.data[['val_acc']])\n",
        "X['bert_trainable'] = pd.factorize(X['bert_trainable'])[0]\n",
        "X['optimizer'] = pd.factorize(X['optimizer'])[0]\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "reg = RandomForestRegressor(max_depth=3,n_estimators=100)\n",
        "reg.fit(X,y)\n",
        "pd.Series(reg.feature_importances_,index=X.columns).\\\n",
        "sort_values(ascending=True).plot.barh(color='grey',title='Feature Importance of Hyperparameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PYsQd-ZASPQE"
      },
      "source": [
        "r.plot_regs(x='acc',y='val_acc')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "92m4kbJySPQE"
      },
      "source": [
        "r.plot_bars(x='vis_no_hidden_layer',y='val_acc',hue='dropout',col='final_no_hidden_layer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_UTImDw4SPQF"
      },
      "source": [
        "r.plot_kde()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DqZ5VCCeSPQF"
      },
      "source": [
        "params = {\n",
        "    'bert_trainable' :[False],\n",
        "    'text_no_hidden_layer':[0,1],\n",
        "    'text_hidden_neurons':[768],\n",
        "    'dropout':[0.4],\n",
        "    'repr_size':[32],\n",
        "    'vis_no_hidden_layer':[1],\n",
        "    'vis_hidden_neurons':[2742],\n",
        "    'final_no_hidden_layer':[0,1],\n",
        "    'final_hidden_neurons':[35],\n",
        "    'optimizer':[tf.keras.optimizers.Adam,tf.keras.optimizers.RMSprop],\n",
        "    'batch_size':[512],\n",
        "    'epochs':[10],\n",
        "    'lr':(0.0001,0.1,10)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sOAAQFFdSPQG"
      },
      "source": [
        "h = ta.Scan([train_input_ids, train_input_masks, train_segment_ids,train_imagesX], trainY, params=params,\n",
        "            model=news_model,\n",
        "            experiment_name='twitter_fake_news',\n",
        "            x_val=[test_input_ids, test_input_masks, test_segment_ids,test_imagesX],\n",
        "            y_val=testY,\n",
        "            fraction_limit=.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ngvus074SPQG"
      },
      "source": [
        "r = ta.Reporting(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EFPW7pyXSPQH"
      },
      "source": [
        "r.high('val_acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2T384Hs0SPQI"
      },
      "source": [
        "r.data.sort_values('val_acc').tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rORgAfXzSPQI"
      },
      "source": [
        "r.plot_corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yVNasKRDSPQJ"
      },
      "source": [
        "for k in params.keys():\n",
        "    print(k)\n",
        "    r.plot_box(y='val_acc',x=k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rM5Wt1lFSPQJ"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X = r.data[['text_no_hidden_layer','final_no_hidden_layer','optimizer','lr']]\n",
        "scaler = MinMaxScaler()\n",
        "y = scaler.fit_transform(r.data[['val_acc']])\n",
        "X['optimizer'] = pd.factorize(X['optimizer'])[0]\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "reg = RandomForestRegressor(max_depth=3,n_estimators=100)\n",
        "reg.fit(X,y)\n",
        "pd.Series(reg.feature_importances_,index=X.columns).\\\n",
        "sort_values(ascending=True).plot.barh(color='grey',title='Feature Importance of Hyperparameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pX03PtrKSPQK"
      },
      "source": [
        "r.plot_regs(x='acc',y='val_acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6U7zFuKmSPQK"
      },
      "source": [
        "r.plot_kde()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aKxzhAv9SPQL"
      },
      "source": [
        "#text model\n",
        "def get_news_model( params):\n",
        "    \n",
        "#     pprint(params)\n",
        "    K.clear_session()\n",
        "    \n",
        "    with tf.device('/cpu:0'):\n",
        "        bert_base = BertLayer()\n",
        "        bert_base.trainable= params['bert_trainable']\n",
        "\n",
        "        in_id = tf.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "        in_mask = tf.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "        in_segment = tf.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "        bert_inputs = [in_id, in_mask, in_segment]\n",
        "        bert_output = bert_base(bert_inputs)\n",
        "\n",
        "        if params['text_no_hidden_layer']>0:\n",
        "            for i in range(params['text_no_hidden_layer']):\n",
        "                bert_output = tf.layers.Dense(params['text_hidden_neurons'], activation='relu')(bert_output)\n",
        "                bert_output = tf.layers.Dropout(params['dropout'])(bert_output)\n",
        "\n",
        "        text_repr = tf.layers.Dense(params['repr_size'], activation='relu')(bert_output)\n",
        "\n",
        "        #image model\n",
        "        conv_base = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "        conv_base.trainable=False\n",
        "#         conv_base = base\n",
        "\n",
        "        input_image = tf.layers.Input(shape=(224,224,3))\n",
        "        base_output = conv_base(input_image)\n",
        "        flat = tf.layers.Flatten()(base_output)\n",
        "\n",
        "        if params['vis_no_hidden_layer']>0:\n",
        "            for i in range(params['vis_no_hidden_layer']):\n",
        "                flat = tf.layers.Dense(params['vis_hidden_neurons'], activation='relu')(flat)\n",
        "                flat = tf.layers.Dropout(params['dropout'])(flat)\n",
        "\n",
        "        visual_repr = tf.layers.Dense(params['repr_size'],activation='relu')(flat)\n",
        "\n",
        "\n",
        "        #classifier\n",
        "        combine_repr = tf.keras.layers.concatenate([text_repr, visual_repr])\n",
        "        com_drop=tf.layers.Dropout(params['dropout'])(combine_repr)\n",
        "\n",
        "        if params['final_no_hidden_layer']>0:\n",
        "            for i in range(params['final_no_hidden_layer']):\n",
        "                com_drop = tf.layers.Dense(params['final_hidden_neurons'], activation='relu')(com_drop)\n",
        "                com_drop=tf.layers.Dropout(params['dropout'])(com_drop)\n",
        "\n",
        "        prediction = tf.layers.Dense(1,activation='sigmoid')(com_drop)\n",
        "\n",
        "        model = tf.keras.models.Model(inputs=[in_id,in_mask,in_segment,input_image], outputs=prediction)\n",
        "\n",
        "  #  model = tf.keras.utils.multi_gpu_model(model,gpus=4)\n",
        "    \n",
        "#     if params['optimizer'] == 'adam':\n",
        "#         opt = tf.keras.optimizers.Adam(lr=0.0005)\n",
        "#     else:\n",
        "#         opt = tf.keras.optimizers.RMSprop(lr=0.00005)\n",
        "        \n",
        "    model.compile(loss='binary_crossentropy', optimizer=params['optimizer'](), metrics=['accuracy'])\n",
        "    initialize_vars(sess)\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CbkFRPYeSPQL"
      },
      "source": [
        "params_adam = {\n",
        "    'bert_trainable' :False,\n",
        "    'text_no_hidden_layer':1,\n",
        "    'text_hidden_neurons':768,\n",
        "    'dropout':0.4,\n",
        "    'repr_size':32,\n",
        "    'vis_no_hidden_layer':1,\n",
        "    'vis_hidden_neurons':2742,\n",
        "    'final_no_hidden_layer':1,\n",
        "    'final_hidden_neurons':35,\n",
        "    'optimizer':tf.keras.optimizers.Adam\n",
        "}\n",
        "\n",
        "params_rms = {\n",
        "    'bert_trainable' :False,\n",
        "    'text_no_hidden_layer':1,\n",
        "    'text_hidden_neurons':768,\n",
        "    'dropout':0.4,\n",
        "    'repr_size':32,\n",
        "    'vis_no_hidden_layer':1,\n",
        "    'vis_hidden_neurons':2742,\n",
        "    'final_no_hidden_layer':1,\n",
        "    'final_hidden_neurons':35,\n",
        "    'optimizer':tf.keras.optimizers.RMSprop\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CtzOs3vUSPQM"
      },
      "source": [
        "model_adam=get_news_model(params_adam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zLOVx375SPQM"
      },
      "source": [
        "lr_finder = LRFinder(model_adam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tWHqD4EMSPQM"
      },
      "source": [
        "lr_finder.find([train_input_ids, train_input_masks, train_segment_ids,train_imagesX], trainY, 0.000001, 0.01, 512, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KVujBzY5SPQN"
      },
      "source": [
        "lr_finder.plot_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fby5UWNcSPQN"
      },
      "source": [
        "model_rms=get_news_model(params_rms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V-LVGRAeSPQO"
      },
      "source": [
        "lr_finder = LRFinder(model_rms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KH2fJ2XOSPQO"
      },
      "source": [
        "lr_finder.find([train_input_ids, train_input_masks, train_segment_ids,train_imagesX], trainY, 0.000001, 0.01, 512, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bAYHuj8TSPQP"
      },
      "source": [
        "lr_finder.plot_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1lSI0ftUSPQP"
      },
      "source": [
        "model_adam=get_news_model(params_adam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "toCsTJpESPQQ"
      },
      "source": [
        "lr_finder = LRFinder(model_adam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6lZvcPQ9SPQQ"
      },
      "source": [
        "lr_finder.find([train_input_ids, train_input_masks, train_segment_ids,train_imagesX], trainY, 0.00005, 0.001, 512, 7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vjj7z8RaSPQR"
      },
      "source": [
        "lr_finder.plot_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UNEodWmiSPQR"
      },
      "source": [
        "model_rms=get_news_model(params_rms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7QyZ7EWtSPQS"
      },
      "source": [
        "lr_finder = LRFinder(model_rms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IWj0PE63SPQS"
      },
      "source": [
        "lr_finder.find([train_input_ids, train_input_masks, train_segment_ids,train_imagesX], trainY, 0.00001, 0.001, 512, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wZahDFDySPQT"
      },
      "source": [
        "lr_finder.plot_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7m71sJRpSPQU"
      },
      "source": [
        "params = {\n",
        "    'bert_trainable' :[False],\n",
        "    'text_no_hidden_layer':[1],\n",
        "    'text_hidden_neurons':[768],\n",
        "    'dropout':[0.4],\n",
        "    'repr_size':[32],\n",
        "    'vis_no_hidden_layer':[1],\n",
        "    'vis_hidden_neurons':[2742],\n",
        "    'final_no_hidden_layer':[1],\n",
        "    'final_hidden_neurons':[35],\n",
        "    'optimizer':[tf.keras.optimizers.Adam],\n",
        "    'batch_size':[128,256,512],\n",
        "    'epochs':[10],\n",
        "    'lr':[0.00005,0.0005,0.00075]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E_ny0GRSSPQU"
      },
      "source": [
        "h = ta.Scan([train_input_ids, train_input_masks, train_segment_ids,train_imagesX], trainY, params=params,\n",
        "            model=news_model,\n",
        "            experiment_name='twitter_fake_news',\n",
        "            #experiment_no='3',\n",
        "            x_val=[test_input_ids, test_input_masks, test_segment_ids,test_imagesX],\n",
        "            y_val=testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CnydiuAFSPQV"
      },
      "source": [
        "r = ta.Reporting(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CHCp8qk2SPQV"
      },
      "source": [
        "r.high('val_acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MmqHQNrOSPQW"
      },
      "source": [
        "r.data.sort_values('val_acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hUK6u7-ZSPQW"
      },
      "source": [
        "for k in ['batch_size','lr']:\n",
        "    print(k)\n",
        "    r.plot_box(y='val_acc',x=k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xn5x1eDoSPQW"
      },
      "source": [
        "r.data.sort_values('val_acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KNlLC49sSPQX"
      },
      "source": [
        "params_final = {\n",
        "    'bert_trainable' :False,\n",
        "    'text_no_hidden_layer':1,\n",
        "    'text_hidden_neurons':768,\n",
        "    'dropout':0.4,\n",
        "    'repr_size':32,\n",
        "    'vis_no_hidden_layer':1,\n",
        "    'vis_hidden_neurons':2742,\n",
        "    'final_no_hidden_layer':1,\n",
        "    'final_hidden_neurons':35,\n",
        "    'optimizer':tf.keras.optimizers.Adam\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Nx0-PoC8SPQX"
      },
      "source": [
        "model=get_news_model(params_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6WIfltofSPQY"
      },
      "source": [
        "K.set_value(model.optimizer.lr, 0.0005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QOHnodgSSPQY"
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model-{epoch:03d}-{val_acc:03f}.h5', verbose=1, monitor='val_acc',save_best_only=True, mode='max')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qiugPqP5SPQZ"
      },
      "source": [
        "out = model.fit([train_input_ids, train_input_masks, train_segment_ids,train_imagesX], trainY,\n",
        "                    batch_size=256,\n",
        "                    epochs=20,\n",
        "                    verbose=0,\n",
        "                    shuffle=True,\n",
        "                    validation_data=([test_input_ids, test_input_masks, test_segment_ids,test_imagesX],testY),callbacks=[live(),checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S7F7C_NwSPQa"
      },
      "source": [
        "model=get_news_model(params_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HK1902dqSPQa"
      },
      "source": [
        "model.load_weights('model-010-0.776923.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JFxEXncHSPQa"
      },
      "source": [
        "test_predict = model.predict([test_input_ids, test_input_masks, test_segment_ids,test_imagesX])\n",
        "test_predict = [1 if i>=0.5 else 0 for i in test_predict]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8wKJZzybSPQb"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AgumYDQdSPQb"
      },
      "source": [
        "print(f'Accuracy == {accuracy_score(testY,test_predict)}')\n",
        "print(f'F1 == {f1_score(testY,test_predict,average=None)}')\n",
        "print(f'Precision == {precision_score(testY,test_predict,average=None)}')\n",
        "print(f'Recall == {recall_score(testY,test_predict,average=None)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B_o9SOMbSPQb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}